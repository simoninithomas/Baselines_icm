import tensorflow as tf
import numpy as np
from baselines.common.mpi_adam_optimizer import MpiAdamOptimizer
from baselines.common.tf_util import get_session, save_variables, load_variables

from mpi4py import MPI
from baselines.common.tf_util import initialize
from baselines.common.mpi_util import sync_from_root

# Helper function to build convnets
def conv2d(inputs, filters, kernel_size, strides, padding):
        return tf.layers.conv2d(inputs = inputs,
                           filters = filters,
                            kernel_size = (kernel_size, kernel_size),
                            strides = strides,
                            padding = padding)


class ICM(object):
    def __init__(self, ob_space, ac_space, max_grad_norm, beta, icm_lr_scale):

        sess = get_session()

        #TODO find a better way
        input_shape = [ob_space.shape[0], ob_space.shape[1], ob_space.shape[2]]
        self.action_shape = 36
            
        # Placeholders
        self.state_ = phi_state = tf.placeholder(tf.float32, [None, *input_shape], name="icm_state")
        self.next_state_ = phi_next_state =  tf.placeholder(tf.float32, [None, *input_shape], name="icm_next_state")
        self.action_ = action = tf.placeholder(tf.float32, [None], name="icm_action")


        with tf.variable_scope('icm_model'):
            # Feature encoding
            # Aka pass state and next_state to create phi(state), phi(next_state)
            # state --> phi(state)
            phi_state = self.feature_encoding(self.state_)

            with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):
                # next_state to phi(next_state)
                phi_next_state = self.feature_encoding(self.next_state_)
            
            # INVERSE MODEL
            pred_actions_logits, pred_actions_prob = self.inverse_model(phi_state, phi_next_state)
            
            # FORWARD MODEL
            pred_phi_next_state = self.forward_model(action, phi_state)


        # CALCULATE THE ICM LOSS
        # Inverse Loss LI
        # We calculate the cross entropy between our ât and at
        # Squeeze the labels (required)
        labels = tf.cast(action, tf.int32)

        self.inv_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred_actions_logits, labels=labels), name="inverse_loss")

        # Foward Loss
        # LF = 1/2 || pred_phi_next_state - phi_next_state ||
        # TODO 0.5 * ?
        self.forw_loss = tf.reduce_mean(tf.square(tf.subtract(pred_phi_next_state, phi_next_state)), name="forward_loss")

        # Todo predictor lr scale ?
        # ICM_LOSS = [(1 - beta) * LI + beta * LF ] * Predictor_Lr_scale
        self.icm_loss = ((1-beta) * self.inv_loss + beta * self.forw_loss) * icm_lr_scale


        # UPDATE THE PARAMETERS USING LOSS
        # 1. Get the model parameters
        icm_params = tf.trainable_variables('icm_model')
        # 2. Build our trainer
        icm_trainer = MpiAdamOptimizer(MPI.COMM_WORLD, learning_rate=1e-4, epsilon=1e-5)
        # 3. Calculate the gradients
        icm_grads_and_var = icm_trainer.compute_gradients(self.icm_loss, icm_params)
        icm_grads, icm_var = zip(*icm_grads_and_var)

        if max_grad_norm is not None:
            # Clip the gradients (normalize)
            icm_grads, icm__grad_norm = tf.clip_by_global_norm(icm_grads, max_grad_norm)
        icm_grads_and_var = list(zip(icm_grads, icm_var))
        # zip aggregate each gradient with parameters associated
        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da

        _icm_train = icm_trainer.apply_gradients(icm_grads_and_var)


        if MPI.COMM_WORLD.Get_rank() == 0:
            print("Initialize")
            initialize()
        global_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope="")
        print("GLOBAL VARIABLES", global_variables)
        sync_from_root(sess, global_variables) #pylint: disable=E1101        
    
    
    # We use batch normalization to do feature normalization as explained in the paper
    def feature_encoding(self, x):
        x = tf.nn.elu(tf.layers.batch_normalization(conv2d(x, 8, 5, 4, "valid")))
        x = tf.nn.elu(tf.layers.batch_normalization(conv2d(x, 16, 3, 2, "valid")))
        x = tf.nn.elu(tf.layers.batch_normalization(conv2d(x, 32, 3, 2, "valid")))
        x = tf.nn.elu(tf.layers.batch_normalization(conv2d(x, 64, 3, 2, "valid")))
        x = tf.layers.flatten(x)
        x = tf.nn.elu(tf.contrib.layers.fully_connected(x, 256))

        return x



    # Inverse Model
    # Given phi(state) and phi(next_state) returns the predicted action ât
    """
    Parameters
    __________
    
    action:   The real action taken by our agent
    phi_state: The feature representation of our state generated by our feature_encoding function.
    phi_next_state: The feature representation of our next_state generated by our feature_encoding function.
    
    returns pred_actions_logits: the logits and pred_actions_prob: the probability distribution of our actions
    """
    def inverse_model(self, phi_state, phi_next_state):
        # Concatenate phi(st) and phi(st+1)
        icm_inv_concatenate = tf.concat([phi_state, phi_next_state], 1)
        icm_inv_fc1 = tf.nn.relu(tf.layers.dense(icm_inv_concatenate, 256))
        pred_actions_logits = tf.layers.dense(icm_inv_fc1, self.action_shape)
        pred_actions_prob = tf.nn.softmax(pred_actions_logits, dim=-1)
        
        return pred_actions_logits, pred_actions_prob
        
    # Foward Model
    # Given action and phi(st) must find pred_phi(st+1)
    """
    Parameters
    __________
    
    action:   The action taken by our agent
    phi_state: The feature representation of our state generated by our feature_encoding function.
    phi_next_state: The feature representation of our next_state generated by our feature_encoding function.
    
    returns pred_phi_next_state: The feature representation prediction of our next_state.
    """
    def forward_model(self, action, phi_state):
        # Concatenate phi_state and action
        action = tf.expand_dims(action, axis=1) # Expand dimension to be able to concatenate

        icm_forw_concatenate = tf.concat(axis=1, values=[phi_state, action])

        # FC
        icm_forw_fc1 = tf.layers.dense(icm_forw_concatenate, 256)

        # FC (size of phi_state [1] aka the width)
        icm_forw_pred_next_state = tf.layers.dense(icm_forw_fc1, phi_state.get_shape()[1].value)

        return icm_forw_pred_next_state
        
    
    # Calculate intrinsic reward
    """
    Parameters
    __________
    
    phi_next_state: The feature representation of our next_state generated by our feature_encoding function.
    pred_phi_next_state:   The feature representation prediction of our next_state.
    
    
    returns intrinsic_reward: The intrinsic reward
    """
    def calculate_intrinsic_reward(self, state, next_state, action):

        sess = tf.get_default_session()

        error = sess.run(self.forw_loss, {self.state_: state, self.next_state_: next_state, self.action_: action})

        error = error * 0.5

        # Return intrinsic reward
        return error



"""



   Need implement train function

        
"""